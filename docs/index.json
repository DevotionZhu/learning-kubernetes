[{"authors":null,"categories":null,"content":"Hi，我是 roc，专注云原生技术领域，不定期分享容器、Kubernetes、Service Mesh 等相关干货。\n（欢迎使用 RSS订阅 本站内容更新）\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi，我是 roc，专注云原生技术领域，不定期分享容器、Kubernetes、Service Mesh 等相关干货。\n（欢迎使用 RSS订阅 本站内容更新）","tags":null,"title":"roc","type":"authors"},{"authors":null,"categories":null,"content":"先占位，后续陆续补充笔记\n","date":1613952000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1613952000,"objectID":"e2ae5d39558e71fe0aebacd371bcafee","permalink":"/learning-kubernetes/kep.html","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-kubernetes/kep.html","section":"kep","summary":"先占位，后续陆续补充笔记","tags":null,"title":"增强特性","type":"book"},{"authors":null,"categories":null,"content":"先占位，后续陆续补充笔记\n","date":1613952000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1613952000,"objectID":"9b683c5fbff8d0d24588e34210d33cb8","permalink":"/learning-kubernetes/faq.html","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-kubernetes/faq.html","section":"faq","summary":"先占位，后续陆续补充笔记","tags":null,"title":"常见问题","type":"book"},{"authors":null,"categories":null,"content":"先占位，后续陆续补充笔记\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":-62135596800,"objectID":"b9e84d1848071c42a33a5ba43e202121","permalink":"/learning-kubernetes/basics.html","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/learning-kubernetes/basics.html","section":"basics","summary":"先占位，后续陆续补充笔记","tags":null,"title":"Kubernetes 基础","type":"book"},{"authors":null,"categories":null,"content":"container 负载不均不扩容 问题描述: K8S HPA 算法是按照 Pod 中所有 container 的资源使用平均值来算的，如果 Pod 中有多个 container，它们的资源使用相差较大，可能导致某个 container 高负载了还不扩容。\n场景举例: istio 场景下，Pod 中有业务 container 和 sidecar container，如果业务 container 的 cpu 100%，而 sidecar container 的 CPU 0%，它们平均下来就是 50%，当 HPA 指定的是 CPU 60% 扩容，这种情况下不会扩容，导致业务高负载还不扩容，影响线上业务。\n解决方案:\n 使用 VPA 根据 container 实际使用的资源大小来动态调整 container 的 request/limit，使得 HPA 计算出来的平均值能够比较客观反映 Pod 整体负载情况。 使用 Container HPA 特性，让 HPA 计算时更加 \u0026ldquo;聪明\u0026rdquo; 一点。  ","date":1613952000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1613952000,"objectID":"930e2630c9d791eee56fd81a21bb26c7","permalink":"/learning-kubernetes/faq/hpa.html","publishdate":"2021-02-22T00:00:00Z","relpermalink":"/learning-kubernetes/faq/hpa.html","section":"faq","summary":"container 负载不均不扩容 问题描述: K8S HPA 算法是按照 Pod 中所有 container 的资源使用平均值来算的，如果 Pod 中有多个 container，它们的资源使用相差较大，可能导致某个 container 高负载了还不扩容。\n场景举例: istio 场景下，Pod 中有业务 container 和 sidecar container，如果业务 container 的 cpu 100%，而 sidecar container 的 CPU 0%，它们平均下来就是 50%，当 HPA 指定的是 CPU 60% 扩容，这种情况下不会扩容，导致业务高负载还不扩容，影响线上业务。","tags":null,"title":"HPA 相关问题","type":"book"},{"authors":null,"categories":null,"content":"背景 K8S 中 Pod 如果有多个 container，正常情况会同时启动或销毁，但有些场景对容器启动或销毁顺序有依赖，就可能存在一些问题，比如在 istio 场景中:\n Pod 启动时: 业务容器比 istio-proxy 先 ready。容器化过渡的应用，业务容器启动时需要调用其它服务(比如从配置中心拉取配置)，如果失败就退出，没有重试逻辑，而当 envoy 启动更慢时，业务容器调用其它服务失败，导致 pod 启动失败，如此循环 (参考 k8s issue #65502 ) 。 Pod 销毁时: 业务容器和 envoy 同时收到 SIGTERM，envoy 不再处理增量连接，但业务容器在 graceful shutdown 过程中可能需要调用另外的服务（比如通知其它清理进行清理操作)，这时候 envoy 就拒绝掉新的请求，导致调用失败 (参考 istio issue #7136 )。   规避方法后续整理 istio 学习笔记\n 发起提案 社区很多人也都遇到了类似的问题，开始有人提出 Proposal 来解决:\n 在 2018-05， Joseph Irving 发起 Sidecar Containers 的 KEP 随后在 2018-11 KEP 被接受 。 接着在 2019-01 作者又新开了 issue #753 来跟进这个特性的进展。  提案被废弃 经过两年的设计与开发，在 2020-10 社区意见出现分歧，最终宣布该 KEP 被废弃，见作者的 评论 。\n还有文章闹过乌龙，称 1.18 会支持 sidecar 特性: Sidecar container lifecycle changes in Kubernetes 1.18  ，但事实证明最终没有，并且还被废弃了。\n原因总结 总结一下原因就是，很多相关问题都是与 pod 生命周期管理有关，涉及很多场景，不仅仅是局限于一两个场景。 我们不能给每种场景都搞一个特性去解决，而是需要由一个能够从更高的高度解决所有问题的新提案来解决。\n讨论新提案 随后，社区发起了 sidecar 相关场景与要求的搜集 Sidecar use cases/requirements ，我印象比较深刻的有:\n Job 运行完毕退出，但 istio sidecar 不会退出，导致 Job 永不退出 (Job 需要等所有 container 停止才算退出) 升级 sidecar 版本会重启所有 Pod，对大集群不友好，能够支持单个 container 升级就好了  然后在 2020-11，Tim Hockin (K8S首席) 发起新 Proposal 草稿。\n最新进展 然后就没有然后了，最近也没发现什么跟这个特性相关的动静，可能是要覆盖众多场景，就需要更复杂的设计，就没那么快能想好\u0026hellip;\n","date":1612224000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1612224000,"objectID":"85c7424625272811ee87168d0fe62b8d","permalink":"/learning-kubernetes/kep/sidecar-containers.html","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/learning-kubernetes/kep/sidecar-containers.html","section":"kep","summary":"背景 K8S 中 Pod 如果有多个 container，正常情况会同时启动或销毁，但有些场景对容器启动或销毁顺序有依赖，就可能存在一些问题，比如在 istio 场景中:\n Pod 启动时: 业务容器比 istio-proxy 先 ready。容器化过渡的应用，业务容器启动时需要调用其它服务(比如从配置中心拉取配置)，如果失败就退出，没有重试逻辑，而当 envoy 启动更慢时，业务容器调用其它服务失败，导致 pod 启动失败，如此循环 (参考 k8s issue #65502 ) 。 Pod 销毁时: 业务容器和 envoy 同时收到 SIGTERM，envoy 不再处理增量连接，但业务容器在 graceful shutdown 过程中可能需要调用另外的服务（比如通知其它清理进行清理操作)，这时候 envoy 就拒绝掉新的请求，导致调用失败 (参考 istio issue #7136 )。   规避方法后续整理 istio 学习笔记","tags":null,"title":"Sidecar Containers","type":"book"},{"authors":null,"categories":null,"content":"历史  2019-12-17: Arjun Naik 提出 issue #86349 。 2020-03-11: 提出 KEP PR #1609 ；发起 issue #1610 来跟进此特性。 2020-04-03: KEP #1609 被接受。  进展  规划在 1.19 进入 alpha，但最终未能实现完，还没合入，见 comment 有望在 1.20 合入，进入 alpha，见 comment  参考资料  特性跟踪 issue: https://github.com/kubernetes/enhancements/issues/1610 KEP 文档: https://github.com/kubernetes/enhancements/tree/master/keps/sig-autoscaling/1610-container-resource-autoscaling  ","date":1613952000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1613952000,"objectID":"868ddb6d4d2eb9486ca9215fd0dd0df2","permalink":"/learning-kubernetes/kep/container-hpa.html","publishdate":"2021-02-22T00:00:00Z","relpermalink":"/learning-kubernetes/kep/container-hpa.html","section":"kep","summary":"历史  2019-12-17: Arjun Naik 提出 issue #86349 。 2020-03-11: 提出 KEP PR #1609 ；发起 issue #1610 来跟进此特性。 2020-04-03: KEP #1609 被接受。  进展  规划在 1.19 进入 alpha，但最终未能实现完，还没合入，见 comment 有望在 1.","tags":null,"title":"Container HPA","type":"book"}]